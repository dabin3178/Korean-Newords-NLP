{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dabin3178/Korean-Newords-NLP/blob/main/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6Fs23ytaa_S"
      },
      "source": [
        "# KoNLPy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSnK3SDCAT6V",
        "outputId": "16f42dda-8ab4-4100-a676-a8357be63093"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSS3wzbTBa13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3c357dd-4383-44a3-e276-da10dcd122d3"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 68.6 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtKhma49WFZu"
      },
      "source": [
        "import konlpy\n",
        "import re\n",
        "okt = konlpy.tag.Okt()\n",
        "\n",
        "def text_preprocess(x): #한글 완성형만 걸러냄.\n",
        "    text=[]\n",
        "    a = re.sub('[^가-힣\\\\s]', '',x)\n",
        "    for j in a.split():\n",
        "        text.append(j)\n",
        "    return ' '.join(text)\n",
        "\n",
        "def dict_preprocess(x): #공백도 제외\n",
        "    text=[]\n",
        "    a = re.sub('[^가-힣]', '',x)\n",
        "    for j in a.split():\n",
        "        text.append(j)\n",
        "    return ' '.join(text)\n",
        "\n",
        "def tokenize(x): # 이거 안 써요.\n",
        "    text = []\n",
        "    tokens = okt.pos(x)\n",
        "    for token in tokens :\n",
        "        if token[1] != 'Noun' or len(token[0]) == 1 or len(token[0]) > 6:\n",
        "            continue\n",
        "        else:\n",
        "            text.append(token[0])\n",
        "    return text\n",
        "\n",
        "\n",
        "def make_ngrams(x): # 부분 글자 만드는 함수\n",
        "    result = []\n",
        "    for i in range(1, len(x) + 1):\n",
        "        ngram = []\n",
        "        for j in range(0, len(x) - i + 1):\n",
        "            ngram.append(x[j:j+i])\n",
        "        result.extend(ngram)\n",
        "\n",
        "    return result\n",
        "\n",
        "def remove_stopwords(sentence, stopwords): # 이거 안 써요\n",
        "  for word in sentence:\n",
        "    if word in stopwords:\n",
        "      sentence.remove(word)\n",
        "  return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS0vF1lNDZui"
      },
      "source": [
        "#무시하는 부분\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "eta_global_df = pd.read_csv(\"/content/drive/MyDrive/Project_FINAL/raw_data/Globalfree.csv\")\n",
        "eta_sisa_df = pd.read_csv(\"/content/drive/MyDrive/Project_FINAL/raw_data/df_에타_sisa.csv\")\n",
        "eta_secret_df = pd.read_csv(\"/content/drive/MyDrive/Project_FINAL/raw_data/eta_비밀.csv\")\n",
        "dc_inside_df = pd.read_csv(\"/content/drive/MyDrive/Project_FINAL/raw_data/df_DC_total.csv\")\n",
        "naver_news_df = pd.read_csv(\"/content/drive/MyDrive/Project_FINAL/raw_data/navernews.csv\")\n",
        "deoku_df = pd.read_csv(\"/content/drive/MyDrive/Project_FINAL/raw_data/deoku.csv\")\n",
        "nate_pann_df = pd.read_csv(\"/content/drive/MyDrive/Project_FINAL/raw_data/네이트판_통합.csv\")\n",
        "ppomppu1_df = pd.read_csv(\"/content/drive/MyDrive/Project_FINAL/raw_data/ppomppu1.csv\")\n",
        "ppomppu2_df = pd.read_csv(\"/content/drive/MyDrive/Project_FINAL/raw_data/ppomppu2.csv\")\n",
        "youtube_df = pd.read_csv(\"/content/drive/MyDrive/Project_FINAL/raw_data/유튜브15개.csv\")\n",
        "\n",
        "total_data_df = pd.concat([eta_global_df, eta_sisa_df],ignore_index=True)\n",
        "total_data_df = pd.concat([total_data_df, eta_secret_df], ignore_index=True)\n",
        "total_data_df = pd.concat([total_data_df, dc_inside_df], ignore_index=True)\n",
        "total_data_df = pd.concat([total_data_df, naver_news_df], ignore_index=True)\n",
        "total_data_df = pd.concat([total_data_df, deoku_df], ignore_index=True)\n",
        "total_data_df = pd.concat([total_data_df, nate_pann_df], ignore_index=True)\n",
        "total_data_df = pd.concat([total_data_df, ppomppu1_df], ignore_index=True)\n",
        "total_data_df = pd.concat([total_data_df, ppomppu2_df], ignore_index=True)\n",
        "total_data_df = pd.concat([total_data_df, youtube_df], ignore_index=True)\n",
        "\n",
        "total_data_df = total_data_df.drop(columns=[\"제목\", \"내용\", \"title\", \"content\", \"vote\", \"Source.Name\", \"Column1\", \"date\", \"Unnamed: 0\", \"url\", \"ID\"], axis=1)\n",
        "\n",
        "total_data_df.to_csv(\"/content/drive/MyDrive/Project_FINAL/total_data.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "total_data_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAMWzbMsBaKV"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Project/total_data.csv\", encoding='utf8', engine='python') # 데이터 불러오기\n",
        "\n",
        "df['process'] = df['comment'].apply(lambda x : text_preprocess(x))  # 정규표현식으로 필요한 부분만 걸러냄.\n",
        "df['process'] = df['process'].apply(lambda x: x.split()) # 어절 단위로 문장을 나눔.\n",
        "\n",
        "words = [] # 여러 어절을 하나의 리스트로 저장할 배열\n",
        "partial_words = [] # 여러 부분 단어를 하나의 리스트로 저장할 배열\n",
        "\n",
        "# 여러 어절을 하나의 리스트로 저장\n",
        "for i in range(len(df['process'])):\n",
        "    words.extend(df['process'][i])\n",
        "\n",
        "# 데이터프레임으로 만들기 위한 형태 변환\n",
        "data = {\n",
        "    'words' : list(set(words))\n",
        "}\n",
        "\n",
        "# 각 어절을 가지고 데이터 프레임을 만듦.\n",
        "new_words_df = pd.DataFrame(data)\n",
        "\n",
        "new_words_df['candidate'] = new_words_df['words'].apply(lambda x: make_ngrams(x)) # 어절에 해당하는 부분 글자를 구함.\n",
        "new_words_df['phoneme_cnt'] = new_words_df['words'].apply(lambda x: len(x)) # 각 어절의 음절 수를 계산.\n",
        "\n",
        "# 여러 부분 단어를 하나의 리스트로 저장\n",
        "for i in range(len(new_words_df['candidate'])):\n",
        "    partial_words.extend(new_words_df['candidate'][i])\n",
        "    \n",
        "\n",
        "word_segment_cnt = {k: 0 for k in range(1, 8)} # 각 어절의 길이에 따른 빈도 분포를 저장할 리스트\n",
        "word_segment_ratio = {k: 0 for k in range(1, 8)} # 각 어절의 길이에 따른 빈도 분포 비율을 저장할 리스트\n",
        "\n",
        "# 각 어절의 길이에 따른 분포를 계산함.\n",
        "for i in range(len(new_words_df['phoneme_cnt'])):\n",
        "    if new_words_df['phoneme_cnt'][i] >= 7:\n",
        "        word_segment_cnt[7] += 1\n",
        "    else:\n",
        "        word_segment_cnt[new_words_df['phoneme_cnt'][i]] += 1\n",
        "\n",
        "total_sum = new_words_df.shape[0]\n",
        "\n",
        "# 각 어절별 빈도 분포 비율을 저장하는 과정\n",
        "for k, v in word_segment_cnt.items():\n",
        "    word_segment_ratio[k] = (v / total_sum) * 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "6HugjfvSHjEA",
        "outputId": "5ff0743e-7812-49cc-e0f3-3d25df1a7c66"
      },
      "source": [
        "new_words_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>candidate</th>\n",
              "      <th>phoneme_cnt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>팩트체크함</td>\n",
              "      <td>[팩, 트, 체, 크, 함, 팩트, 트체, 체크, 크함, 팩트체, 트체크, 체크함,...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>상향</td>\n",
              "      <td>[상, 향, 상향]</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>싸웠다는거</td>\n",
              "      <td>[싸, 웠, 다, 는, 거, 싸웠, 웠다, 다는, 는거, 싸웠다, 웠다는, 다는거,...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>묘목기부</td>\n",
              "      <td>[묘, 목, 기, 부, 묘목, 목기, 기부, 묘목기, 목기부, 묘목기부]</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>폭투</td>\n",
              "      <td>[폭, 투, 폭투]</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>635495</th>\n",
              "      <td>순간이지만</td>\n",
              "      <td>[순, 간, 이, 지, 만, 순간, 간이, 이지, 지만, 순간이, 간이지, 이지만,...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>635496</th>\n",
              "      <td>혈압오를짤</td>\n",
              "      <td>[혈, 압, 오, 를, 짤, 혈압, 압오, 오를, 를짤, 혈압오, 압오를, 오를짤,...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>635497</th>\n",
              "      <td>학폭설</td>\n",
              "      <td>[학, 폭, 설, 학폭, 폭설, 학폭설]</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>635498</th>\n",
              "      <td>영선좌가</td>\n",
              "      <td>[영, 선, 좌, 가, 영선, 선좌, 좌가, 영선좌, 선좌가, 영선좌가]</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>635499</th>\n",
              "      <td>피해없도록</td>\n",
              "      <td>[피, 해, 없, 도, 록, 피해, 해없, 없도, 도록, 피해없, 해없도, 없도록,...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>635500 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        words                                          candidate  phoneme_cnt\n",
              "0       팩트체크함  [팩, 트, 체, 크, 함, 팩트, 트체, 체크, 크함, 팩트체, 트체크, 체크함,...            5\n",
              "1          상향                                         [상, 향, 상향]            2\n",
              "2       싸웠다는거  [싸, 웠, 다, 는, 거, 싸웠, 웠다, 다는, 는거, 싸웠다, 웠다는, 다는거,...            5\n",
              "3        묘목기부           [묘, 목, 기, 부, 묘목, 목기, 기부, 묘목기, 목기부, 묘목기부]            4\n",
              "4          폭투                                         [폭, 투, 폭투]            2\n",
              "...       ...                                                ...          ...\n",
              "635495  순간이지만  [순, 간, 이, 지, 만, 순간, 간이, 이지, 지만, 순간이, 간이지, 이지만,...            5\n",
              "635496  혈압오를짤  [혈, 압, 오, 를, 짤, 혈압, 압오, 오를, 를짤, 혈압오, 압오를, 오를짤,...            5\n",
              "635497    학폭설                             [학, 폭, 설, 학폭, 폭설, 학폭설]            3\n",
              "635498   영선좌가           [영, 선, 좌, 가, 영선, 선좌, 좌가, 영선좌, 선좌가, 영선좌가]            4\n",
              "635499  피해없도록  [피, 해, 없, 도, 록, 피해, 해없, 없도, 도록, 피해없, 해없도, 없도록,...            5\n",
              "\n",
              "[635500 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzmAdumFD7lZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cce92831-2126-4dc5-b923-2be10f199b04"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "freq = [0] * len(partial_words)\n",
        "\n",
        "for i in tqdm(range(len(partial_words))):\n",
        "    for j in range(len(words)):\n",
        "        if partial_words[i] in words[j]:\n",
        "            freq[i] += 1\n",
        "\n",
        "freq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1042/9070531 [10:08<1464:43:55,  1.72it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW0YDw0r9xaC"
      },
      "source": [
        "# 부분 단어를 데이터 프레임으로 변환하기 위한 형태 변환\n",
        "partial_data = {\n",
        "    'partial_words': partial_words\n",
        "}\n",
        "\n",
        "# 데이터 프레임 생성\n",
        "partial_words_df = pd.DataFrame(partial_data)\n",
        "\n",
        "freq_ratio = [] # 부분 단어의 빈도 비율을 저장할 리스트\n",
        "\n",
        "total_freq = sum(list(freq))\n",
        "\n",
        "# 부분 단어의 빈도 비율 계산\n",
        "for i in range(len(freq)):\n",
        "    freq_ratio.append((freq[i]/total_freq)*100)\n",
        "\n",
        "# 새롭게 부분 단어와 빈도 관련 데이터 프레임 생성 위함.\n",
        "partial_data = {\n",
        "    'partial_words': partial_words.copy(),\n",
        "    'freq': freq,\n",
        "    'freq_ratio': freq_ratio\n",
        "}\n",
        "\n",
        "# 데이터 프레임 생성.\n",
        "partial_words_df = pd.DataFrame(partial_data, index=[i for i in range(0, len(freq))])\n",
        "\n",
        "partial_words_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNudK2QRQ8Be"
      },
      "source": [
        "total_freq, sum(partial_words_df['freq']) # 실제 전체 빈도수가 잘 계산 됐는지 확인."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yinWt2TE2bDv"
      },
      "source": [
        "new_words_df.shape # 전체 어절의 개수"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HFY9_y7EXLU"
      },
      "source": [
        "word_segment_cnt, sum(word_segment_cnt.values()) # 총 어절의 음절 길이에 따른 빈도 수"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2lhUmLWLDRv"
      },
      "source": [
        "word_segment_ratio # 총 어절의 음절 길이에 따른 빈도 분포 비율"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CmP0XMx2aGA"
      },
      "source": [
        "# 불필요한 정보를 걸러 저장하기 위한 리스트들\n",
        "partial_words_extracted = []\n",
        "freq_extracted = []\n",
        "freq_ratio_extracted = []\n",
        "\n",
        "freq_criteria = 20 # 걸러내지 않을 최소 등장 빈도 수\n",
        "\n",
        "# 부분 단어의 길이가 1 혹은 7 이상, 아니면 그 부분 단어의 등장 빈도수가 기준 빈도수보다 낮으면 걸러냄.\n",
        "for i in range(len(partial_words_df['freq'])):\n",
        "    if partial_words_df['freq'][i] >= freq_criteria and (len(partial_words_df['partial_words'][i]) > 1  and len(partial_words_df['partial_words'][i]) < 7) :\n",
        "        partial_words_extracted.append(partial_words_df['partial_words'][i])\n",
        "        freq_extracted.append(partial_words_df['freq'][i])\n",
        "        freq_ratio_extracted.append(partial_words_df['freq_ratio'][i])\n",
        "\n",
        "# 데이터 프레임 생성을 위한 딕셔너리\n",
        "partial_data = {\n",
        "    'partial_words': partial_words_extracted,\n",
        "    'freq': freq_extracted,\n",
        "    'freq_ratio': freq_ratio_extracted\n",
        "}\n",
        "# 데이터 프레임 생성\n",
        "partial_words_df = pd.DataFrame(partial_data)\n",
        "\n",
        "partial_words_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxdWMoxKN4wn"
      },
      "source": [
        "partial_words_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka7IlaqRmPAN"
      },
      "source": [
        "# 국어사전에서 단어를 불러오고 확인.\n",
        "official_dict = pd.read_csv(\"/content/drive/MyDrive/Project_FINAL/dict/우리말샘_어활검품2.csv\")\n",
        "official_dict = list(official_dict['어휘'])\n",
        "\n",
        "for i in range(len(official_dict)):\n",
        "    official_dict[i] = dict_preprocess(official_dict[i])\n",
        "\n",
        "tmp = []\n",
        "for i in range(len(official_dict)):\n",
        "    if official_dict[i] != '':\n",
        "        tmp.append(official_dict[i])\n",
        "\n",
        "official_dict = tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4CbZuVra_XE"
      },
      "source": [
        "official_data = {\n",
        "    \"어휘\": official_dict\n",
        "\n",
        "}\n",
        "\n",
        "official_dict_df = pd.DataFrame(official_data)\n",
        "official_dict_df.to_csv(\"/content/drive/MyDrive/Project_FINAL/dict/우리말샘_전처리.csv\", encoding=\"utf-8-sig\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOPr7H-6xKM2",
        "outputId": "5495c4eb-ff87-4c64-90e0-326b561b6c3d"
      },
      "source": [
        "# 국어사전과 비교하여 사전에 없는 단어만 뽑아내어 저장할 리스트\n",
        "partial_words_extracted = []\n",
        "freq_extracted = []\n",
        "freq_ratio_extracted = []\n",
        "\n",
        "# 문자열 인덱싱을 위한 사전의 딕셔너리 형태 변환.\n",
        "official_dict = {official_dict[i]: \"\" for i in range(len(official_dict))}\n",
        "\n",
        "for i in range(len(partial_words_df['partial_words'])):\n",
        "    try:\n",
        "        official_dict[partial_words_df['partial_words'][i]] # 사전에 있는지 확인.\n",
        "    except KeyError: # 없으면 저장.\n",
        "        partial_words_extracted.append(partial_words_df['partial_words'][i])\n",
        "        freq_extracted.append(partial_words_df['freq'][i])\n",
        "        freq_ratio_extracted.append(partial_words_df['freq_ratio'][i])\n",
        "\n",
        "# 데이터 프레임 변환을 위한 딕셔너리\n",
        "partial_data = {\n",
        "    'partial_words': partial_words_extracted,\n",
        "    'freq': freq_extracted,\n",
        "    'freq_ratio': freq_ratio_extracted,\n",
        "}\n",
        "# 데이터 프레임 생성\n",
        "partial_words_df = pd.DataFrame(partial_data)\n",
        "\n",
        "partial_words_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50462, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gZaSWNI1sqQ"
      },
      "source": [
        "new_words_df.to_csv(\"/content/drive/MyDrive/Project_FINAL/ngrams.csv\", encoding='utf-8-sig', index=False) # 어절에 관한 정보"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0lWqD4xe1oS"
      },
      "source": [
        "partial_words_df.to_csv(\"/content/drive/MyDrive/Project_FINAL/partial_total.csv\", encoding='utf-8-sig', index=False) # 부분 단어에 대한 정보"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv55-Q00C3FU"
      },
      "source": [
        "df.to_csv(\"/content/drive/MyDrive/Project_FINAL/origin_preprocessed.csv\", encoding='utf-8-sig', index=False) # 원본 문장에 대한 전처리에 대한 정보"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW4L5kj2coY6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "2d66b273-dd57-4d76-9fd0-1f1230f14f22"
      },
      "source": [
        "adf = pd.read_csv(\"/content/drive/MyDrive/Project_FINAL/partial_total.csv\")\n",
        "adf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>partial_words</th>\n",
              "      <th>freq</th>\n",
              "      <th>freq_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>하는</td>\n",
              "      <td>42350</td>\n",
              "      <td>0.162095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>는데</td>\n",
              "      <td>30803</td>\n",
              "      <td>0.117899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>진짜</td>\n",
              "      <td>22045</td>\n",
              "      <td>0.084377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>는거</td>\n",
              "      <td>18113</td>\n",
              "      <td>0.069328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>림픽</td>\n",
              "      <td>13087</td>\n",
              "      <td>0.050091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50457</th>\n",
              "      <td>스터라</td>\n",
              "      <td>20</td>\n",
              "      <td>0.000077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50458</th>\n",
              "      <td>산다고</td>\n",
              "      <td>20</td>\n",
              "      <td>0.000077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50459</th>\n",
              "      <td>본인을</td>\n",
              "      <td>20</td>\n",
              "      <td>0.000077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50460</th>\n",
              "      <td>답일</td>\n",
              "      <td>20</td>\n",
              "      <td>0.000077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50461</th>\n",
              "      <td>잘봤습니다</td>\n",
              "      <td>20</td>\n",
              "      <td>0.000077</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50462 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      partial_words   freq  freq_ratio\n",
              "0                하는  42350    0.162095\n",
              "1                는데  30803    0.117899\n",
              "2                진짜  22045    0.084377\n",
              "3                는거  18113    0.069328\n",
              "4                림픽  13087    0.050091\n",
              "...             ...    ...         ...\n",
              "50457           스터라     20    0.000077\n",
              "50458           산다고     20    0.000077\n",
              "50459           본인을     20    0.000077\n",
              "50460            답일     20    0.000077\n",
              "50461         잘봤습니다     20    0.000077\n",
              "\n",
              "[50462 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qWMkH3r1Mhp"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}